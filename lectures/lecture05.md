### Lecture: 5. MLP, Softmax as MaxEnt classifier, F1 score
#### Date: Oct 27, Oct 30
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2526/slides/?05
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2526/npfl129-2526-05-czech.mp4, CS Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2526/npfl129-2526-05-english.mp4, EN Lecture
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2526/slides.pdf/npfl129-2526-05.pdf,PDF Slides
#### Lecture assignment: multilabel_classification_sgd
#### Lecture assignment: diacritization
#### Questions: #lecture_5_questions

**Learning objectives.** After the lecture you should be able to

- Implement training of multi-layer perceptron using SGD.
- Explain the theoretical foundation behind the softmax activation function
  (including the necessary math).
- Choose a suitable evaluation metric for various classification tasks.

**Covered topics** and where to find more:

- Lagrange multipliers [Appendix E of PRML]
  - [A Simple Explanation of Why Lagrange Multipliers Works](https://medium.com/@andrew.chamberlain/a-simple-explanation-of-why-lagrange-multipliers-works-253e2cdcbf74), a blog post by Andrew Chamberlain
- Derivation of softmax via the maximum entropy principle [[The equivalence of logistic regression and maximum entropy models writeup](https://github.com/WinVector/Examples/blob/main/dfiles/LogisticRegressionMaxEnt.pdf)]
- $F_1$ score and $F_Î²$ score
- [IPython notebook on micro and macro F1-score](https://github.com/ufal/npfl129/blob/master/notebooks/confusion_matrix.ipynb)
